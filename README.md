<div align="center">

# LLVC : **L**ow-Latency **L**ow-Resource **V**oice **C**onversion <!-- omit in toc -->
[![OpenInColab]][notebook]
[![paper_badge]][paper]

</div>

Clone of the official ***LLVC*** implementation.  

<!-- Auto-generated by "Markdown All in One" extension -->
- [Demo](#demo)
- [Usage](#usage)
  - [Install](#install)
  - [Train](#train)
  - [Inference](#inference)
- [Results](#results)
- [References](#references)

## Demo
[Official demo][demo page].  
[Windows executable](https://koe.ai/recast/download/).  

## Usage
### Install
On Python3.11 environment:  

```bash
# pip install "torch==x.y.z" -q      # Based on your environment (validated with vX.YZ)
# pip install "torchaudio==x.y.z" -q # Based on your environment
pip install -r requirements.txt
python download_models.py # Download pretrained models from huggingface
```

#### `eval.py`
Special environment for evaluation.

On (another, separated) Python3.9 environment:  

```bash
# pip install "torch==x.y.z" -q      # Based on your environment (validated with vX.YZ)
# pip install "torchaudio==x.y.z" -q # Based on your environment
pip install -r eval_requirements.txt
```

### Inference
Run A2O-VC with LLVC:  

```bash
python infer.py
```
default behavior: `test_wavs/xxx.wav` -> (A2O-VC with pretrained llvc checkpoint) -> `converted_out/xxx.wav`  
For options, check `python infer.py --help`.  
In short, `infer.py` support 'model variant', 'original checkpoint', 'I/O dir', 'streamingSim' and 'chunk_factor'.  


Run VC with streaming no-f0 RVC / QuickVC (as in the paper):  

```bash
python compare_infer.py
```
 By default, `window_ms` and `extra_convert_size` are set to the values used for no-f0 RVC conversion. See the linked paper for the QuickVC conversion parameters.


## Train
1. Create a folder `experiments/my_run` containing a `config.json` (see `experiments/llvc/config.json` for an example)
2. Edit the `config.json` to reflect the location of your dataset and desired architectural modifications
3. `python train.py -d experiments/my_run`
4. The run will be logged to Tensorboard in the directory `experiments/my_run/logs`

### Evaluate results
1. Download test-clean.tar.gz from https://www.openslr.org/12
2. Use `infer.py` to convert the test-clean folder using the checkpoint that you want to evaluate
3. Activate the eval environment and run `eval.py` on your converted audio and directory of ground-truth audio files.

## Dataset
Datasets are comprised of a folder containing three subfolders: `dev`, `train` and `val`. Each of these folders contains audio files of the form `PREFIX_original.wav`, which are audio clips recorded by a variety of input speakers, and `PREFIX_converted.wav`, which are the original audio clips converted to a single target speaker. `val` contains clips from the same speakers as `test`. `dev` contains clips from different speakers than `test`. 

To recreate the dataset that we use in our paper:
1. Download dev-clean.tar.gz and train-clean-360.tar.gz from https://www.openslr.org/12 and unzip to `llvc/LibriSpeech`
2. 
```
python -m minimal_rvc._infer_folder \
                                    --train_set_path "LibriSpeech/train-clean-360" \
                                    --dev_set_path "LibriSpeech/dev-clean" \
                                    --out_path "f_8312_ls360" \
                                    --flatten \
                                    --model_path "llvc_models/models/rvc/f_8312_32k-325.pth" \
                                    --model_name "f_8312" \
                                    --target_sr 16000 \
                                    --f0_method "rmvpe" \
                                    --val_percent 0.02 \
                                    --random_seed 42 \
                                    --f0_up_key 12
```

## Results
### Sample <!-- omit in toc -->
[Demo](#demo)

### Performance <!-- omit in toc -->
- training
  - x.x [iter/sec] @ NVIDIA X0 on Google Colaboratory (AMP+)
  - take about y days for whole training
- inference
  - z.z [sec/sample] @ xx

## References
### Original paper <!-- omit in toc -->
[![paper_badge]][paper]  
<!-- Generated with the tool -> https://arxiv2bibtex.org/?q=2311.00873&format=bibtex -->
```bibtex
@misc{2311.00873,
Author = {Konstantine Sadov and Matthew Hutter and Asara Near},
Title = {Low-latency Real-time Voice Conversion on CPU},
Year = {2023},
Eprint = {arXiv:2311.00873},
}
```

### Acknowlegements <!-- omit in toc -->
Many of the modules written in `minimal_rvc/` are based on the following repositories:
- https://github.com/ddPn08/rvc-webui
- https://github.com/RVC-Project/Retrieval-based-Voice-Conversion-WebUI
- https://github.com/teftef6220/Voice_Separation_and_Selection


[paper]: https://arxiv.org/abs/2311.00873
[paper_badge]: http://img.shields.io/badge/paper-arxiv.2311.00873-B31B1B.svg
[notebook]: https://colab.research.google.com/github/tarepan/LLVC-official/blob/main/llvc.ipynb
[OpenInColab]: https://colab.research.google.com/assets/colab-badge.svg
[demo page]: https://koeai.github.io/llvc-demo/